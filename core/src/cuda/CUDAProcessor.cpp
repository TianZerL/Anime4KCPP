#include <cassert>
#include <cstddef>
#include <memory>
#include <sstream>
#include <string>
#include <vector>

#include <cuda_runtime.h>

#include "AC/Core/Half.hpp"
#include "AC/Core/Model.hpp"
#include "AC/Core/Processor.hpp"
#include "AC/Util/ThreadLocal.hpp"

#include "ACExport.hpp" // Generated by CMake

#define ContextList (ac::core::cuda::getContextList())

namespace ac::core::cuda
{
    void conv3x3_1to8_cuda(
        cudaTextureObject_t src,
        cudaSurfaceObject_t dst,
        unsigned int width,
        unsigned int height,
        const void* kernels,
        std::size_t koffset,
        const void* biases,
        std::size_t boffset,
        int computeCapability,
        cudaStream_t stream
    ) noexcept;
    void conv3x3_8to8_cuda(
        cudaSurfaceObject_t src,
        cudaSurfaceObject_t dst,
        unsigned int width,
        unsigned int height,
        const void* kernels,
        std::size_t koffset,
        const void* biases,
        std::size_t boffset,
        int computeCapability,
        cudaStream_t stream
    ) noexcept;
    void conv3x3_residual_8to8_cuda(
        cudaSurfaceObject_t src,
        cudaSurfaceObject_t dst,
        unsigned int width,
        unsigned int height,
        const void* kernels,
        std::size_t koffset,
        const void* biases,
        std::size_t boffset,
        int computeCapability,
        cudaStream_t stream
    ) noexcept;
    void deconv2x2_8to1_cuda(
        cudaSurfaceObject_t src,
        cudaSurfaceObject_t dst,
        unsigned int width,
        unsigned int height,
        const void* kernels,
        std::size_t koffset,
        Image::ElementType type,
        int computeCapability,
        cudaStream_t stream
    ) noexcept;

    struct Context
    {
        std::string name{};
        std::size_t vram{};
        int computeCapability{};
    };

    //lazy load, god knows if it's safe to call the cuda function during DLL initialization
    inline static std::vector<Context>& getContextList() noexcept
    {
        static auto contextList = []() -> std::vector<Context> {
            std::vector<Context> contexts{};
            int deviceCount = 0;
            cudaGetDeviceCount(&deviceCount);
            for (int i = 0; i < deviceCount; i++)
            {
                cudaDeviceProp deviceProp{};
                cudaGetDeviceProperties(&deviceProp, i);
                int computeCapability = deviceProp.major * 10 + deviceProp.minor;
                contexts.emplace_back(Context{ deviceProp.name, (deviceProp.totalGlobalMem >> 20), computeCapability });
            }
            return contexts;
        }();
        return contextList;
    }

    inline static cudaChannelFormatDesc channelType(const Image::ElementType elementType) noexcept
    {
        switch (elementType)
        {
        case Image::UInt8: return cudaCreateChannelDesc<std::uint8_t>();
        case Image::UInt16: return cudaCreateChannelDesc<std::uint16_t>();
        case Image::Float32: return cudaCreateChannelDesc<float>();
        default: return assert(elementType == Image::UInt8 || elementType == Image::UInt16 || elementType == Image::Float32), cudaChannelFormatDesc{};
        }
    }

    class CUDAProcessorBase : public Processor
    {
    public:
        CUDAProcessorBase(const int device) noexcept
        {
            idx = (device >= 0 && static_cast<decltype(ContextList.size())>(device) < ContextList.size()) ? device : 0;
            computeCapability = ContextList[idx].computeCapability;
        };
        ~CUDAProcessorBase() noexcept override = default;

        bool ok() noexcept override
        {
            return errors.local() == cudaSuccess;
        }
        const char* error() noexcept override
        {
            return cudaGetErrorString(errors.local());
        }
        const char* name() const noexcept override
        {
            return ContextList[idx].name.c_str();
        }
    protected:
        int computeCapability{};
        util::ThreadLocal<cudaError_t> errors{};
    };

    template<typename Model>
    class CUDAProcessorSeqCNN : public CUDAProcessorBase
    {
    public:
        CUDAProcessorSeqCNN(const int device, const Model& model) noexcept : CUDAProcessorBase(device)
        {
            auto& err = errors.local();
            err = cudaSetDevice(idx); if (err != cudaSuccess) return;
            if (computeCapability >= 70)
            {
                err = cudaMalloc(&kernels, model.kernelSize<ac::core::Half>()); if (err != cudaSuccess) return;
                err = cudaMalloc(&biases, model.biasSize<ac::core::Half>()); if (err != cudaSuccess) return;
                err = cudaMemcpy(kernels, model.kernels<ac::core::Half>(), model.kernelSize<ac::core::Half>(), cudaMemcpyHostToDevice); if (err != cudaSuccess) return;
                err = cudaMemcpy(biases, model.biases<ac::core::Half>(), model.biasSize<ac::core::Half>(), cudaMemcpyHostToDevice);
            }
            else
            {
                err = cudaMalloc(&kernels, model.kernelSize<float>()); if (err != cudaSuccess) return;
                err = cudaMalloc(&biases, model.biasSize<float>()); if (err != cudaSuccess) return;
                err = cudaMemcpy(kernels, model.kernels<float>(), model.kernelSize<float>(), cudaMemcpyHostToDevice); if (err != cudaSuccess) return;
                err = cudaMemcpy(biases, model.biases<float>(), model.biasSize<float>(), cudaMemcpyHostToDevice);
            }
        }
        ~CUDAProcessorSeqCNN() noexcept override
        {
            cudaSetDevice(idx);

            if (kernels) cudaFree(kernels);
            if (biases) cudaFree(biases);
        }

    protected:
        void* kernels = nullptr;
        void* biases = nullptr;
    };

    template<typename Model>
    class CUDAProcessor;
}

template<>
class ac::core::cuda::CUDAProcessor<ac::core::model::ACNet> : public CUDAProcessorSeqCNN<model::ACNet>
{
public:
    CUDAProcessor(int device, const model::ACNet& model) noexcept;
    ~CUDAProcessor() noexcept override;

private:
    void process(const Image& src, Image& dst) override;
};

ac::core::cuda::CUDAProcessor<ac::core::model::ACNet>::CUDAProcessor(const int device, const model::ACNet& model) noexcept : CUDAProcessorSeqCNN(device, model) {}
ac::core::cuda::CUDAProcessor<ac::core::model::ACNet>::~CUDAProcessor() noexcept = default;

void ac::core::cuda::CUDAProcessor<ac::core::model::ACNet>::process(const Image& src, Image& dst)
{
    cudaSetDevice(idx);

    auto stream = cudaStreamPerThread;

    auto srcW = src.width(), srcH = src.height();
    auto dstW = dst.width(), dstH = dst.height();
    auto srcWBytes = srcW * src.pixelSize();
    auto dstWBytes = dstW * dst.pixelSize();

    auto imgDesc = channelType(src.type());
    auto tmpDesc = cudaCreateChannelDescHalf4();

    cudaArray_t inArray{};
    cudaArray_t tmp1Array{};
    cudaArray_t tmp2Array{};
    cudaArray_t outArray{};

    cudaMallocArray(&inArray, &imgDesc, srcW, srcH);
    cudaMalloc3DArray(&tmp1Array, &tmpDesc, make_cudaExtent(srcW, srcH, 2), cudaArraySurfaceLoadStore | cudaArrayLayered);
    cudaMalloc3DArray(&tmp2Array, &tmpDesc, make_cudaExtent(srcW, srcH, 2), cudaArraySurfaceLoadStore | cudaArrayLayered);
    cudaMallocArray(&outArray, &imgDesc, dstW, dstH, cudaArraySurfaceLoadStore);

    cudaResourceDesc resDesc{};
    cudaTextureDesc texDesc{};

    resDesc.resType = cudaResourceTypeArray;
    texDesc.addressMode[0] = cudaAddressModeBorder;
    texDesc.addressMode[1] = cudaAddressModeBorder;
    texDesc.addressMode[2] = cudaAddressModeBorder;
    texDesc.filterMode = cudaFilterModePoint;
    texDesc.normalizedCoords = false;
    texDesc.readMode = src.isFloat() ? cudaReadModeElementType : cudaReadModeNormalizedFloat;

    cudaTextureObject_t in{};
    resDesc.res.array.array = inArray;
    cudaCreateTextureObject(&in, &resDesc, &texDesc, nullptr);

    texDesc.readMode = cudaReadModeElementType;

    cudaSurfaceObject_t tmp1{};
    resDesc.res.array.array = tmp1Array;
    cudaCreateSurfaceObject(&tmp1, &resDesc);

    cudaSurfaceObject_t tmp2{};
    resDesc.res.array.array = tmp2Array;
    cudaCreateSurfaceObject(&tmp2, &resDesc);

    cudaSurfaceObject_t out{};
    resDesc.res.array.array = outArray;
    cudaCreateSurfaceObject(&out, &resDesc);

    cudaMemcpy2DToArrayAsync(inArray, 0, 0, src.ptr(), src.stride(), srcWBytes, srcH, cudaMemcpyHostToDevice, stream);

    conv3x3_1to8_cuda(in, tmp1, srcW, srcH, kernels, model::ACNet::kernelOffset[0], biases, model::ACNet::baisOffset[0], computeCapability, stream);
    conv3x3_8to8_cuda(tmp1, tmp2, srcW, srcH, kernels, model::ACNet::kernelOffset[1], biases, model::ACNet::baisOffset[1], computeCapability, stream);
    conv3x3_8to8_cuda(tmp2, tmp1, srcW, srcH, kernels, model::ACNet::kernelOffset[2], biases, model::ACNet::baisOffset[2], computeCapability, stream);
    conv3x3_8to8_cuda(tmp1, tmp2, srcW, srcH, kernels, model::ACNet::kernelOffset[3], biases, model::ACNet::baisOffset[3], computeCapability, stream);
    conv3x3_8to8_cuda(tmp2, tmp1, srcW, srcH, kernels, model::ACNet::kernelOffset[4], biases, model::ACNet::baisOffset[4], computeCapability, stream);
    conv3x3_8to8_cuda(tmp1, tmp2, srcW, srcH, kernels, model::ACNet::kernelOffset[5], biases, model::ACNet::baisOffset[5], computeCapability, stream);
    conv3x3_8to8_cuda(tmp2, tmp1, srcW, srcH, kernels, model::ACNet::kernelOffset[6], biases, model::ACNet::baisOffset[6], computeCapability, stream);
    conv3x3_8to8_cuda(tmp1, tmp2, srcW, srcH, kernels, model::ACNet::kernelOffset[7], biases, model::ACNet::baisOffset[7], computeCapability, stream);
    conv3x3_8to8_cuda(tmp2, tmp1, srcW, srcH, kernels, model::ACNet::kernelOffset[8], biases, model::ACNet::baisOffset[8], computeCapability, stream);
    deconv2x2_8to1_cuda(tmp1, out, dstW, dstH, kernels, model::ACNet::kernelOffset[9], dst.type(), computeCapability, stream);

    cudaMemcpy2DFromArrayAsync(dst.ptr(), dst.stride(), outArray, 0, 0, dstWBytes, dstH, cudaMemcpyDeviceToHost, stream);

    cudaStreamSynchronize(stream);

    cudaDestroyTextureObject(in);
    cudaDestroySurfaceObject(tmp1);
    cudaDestroySurfaceObject(tmp2);
    cudaDestroySurfaceObject(out);

    cudaFreeArray(inArray);
    cudaFreeArray(tmp1Array);
    cudaFreeArray(tmp2Array);
    cudaFreeArray(outArray);

    errors.local() = cudaGetLastError();
}

template<>
AC_EXPORT std::shared_ptr<ac::core::Processor> ac::core::Processor::create<ac::core::Processor::CUDA, ac::core::model::ACNet>(const int idx, const model::ACNet& model)
{
    return std::make_shared<cuda::CUDAProcessor<model::ACNet>>(idx, model);
}


template<>
class ac::core::cuda::CUDAProcessor<ac::core::model::ARNet> : public CUDAProcessorSeqCNN<model::ARNet>
{
public:
    CUDAProcessor(int device, const model::ARNet& model) noexcept;
    ~CUDAProcessor() noexcept override;

private:
    void process(const Image& src, Image& dst) override;
};

ac::core::cuda::CUDAProcessor<ac::core::model::ARNet>::CUDAProcessor(const int device, const model::ARNet& model) noexcept : CUDAProcessorSeqCNN(device, model) {}
ac::core::cuda::CUDAProcessor<ac::core::model::ARNet>::~CUDAProcessor() noexcept = default;

void ac::core::cuda::CUDAProcessor<ac::core::model::ARNet>::process(const Image& src, Image& dst)
{
    cudaSetDevice(idx);

    auto stream = cudaStreamPerThread;

    auto srcW = src.width(), srcH = src.height();
    auto dstW = dst.width(), dstH = dst.height();
    auto srcWBytes = srcW * src.pixelSize();
    auto dstWBytes = dstW * dst.pixelSize();

    auto imgDesc = channelType(src.type());
    auto tmpDesc = cudaCreateChannelDescHalf4();

    cudaArray_t inArray{};
    cudaArray_t tmp1Array{};
    cudaArray_t tmp2Array{};
    cudaArray_t outArray{};

    cudaMallocArray(&inArray, &imgDesc, srcW, srcH);
    cudaMalloc3DArray(&tmp1Array, &tmpDesc, make_cudaExtent(srcW, srcH, 2), cudaArraySurfaceLoadStore | cudaArrayLayered);
    cudaMalloc3DArray(&tmp2Array, &tmpDesc, make_cudaExtent(srcW, srcH, 2), cudaArraySurfaceLoadStore | cudaArrayLayered);
    cudaMallocArray(&outArray, &imgDesc, dstW, dstH, cudaArraySurfaceLoadStore);

    cudaResourceDesc resDesc{};
    cudaTextureDesc texDesc{};

    resDesc.resType = cudaResourceTypeArray;
    texDesc.addressMode[0] = cudaAddressModeBorder;
    texDesc.addressMode[1] = cudaAddressModeBorder;
    texDesc.addressMode[2] = cudaAddressModeBorder;
    texDesc.filterMode = cudaFilterModePoint;
    texDesc.normalizedCoords = false;
    texDesc.readMode = src.isFloat() ? cudaReadModeElementType : cudaReadModeNormalizedFloat;

    cudaTextureObject_t in{};
    resDesc.res.array.array = inArray;
    cudaCreateTextureObject(&in, &resDesc, &texDesc, nullptr);

    texDesc.readMode = cudaReadModeElementType;

    cudaSurfaceObject_t tmp1{};
    resDesc.res.array.array = tmp1Array;
    cudaCreateSurfaceObject(&tmp1, &resDesc);

    cudaSurfaceObject_t tmp2{};
    resDesc.res.array.array = tmp2Array;
    cudaCreateSurfaceObject(&tmp2, &resDesc);

    cudaSurfaceObject_t out{};
    resDesc.res.array.array = outArray;
    cudaCreateSurfaceObject(&out, &resDesc);

    cudaMemcpy2DToArrayAsync(inArray, 0, 0, src.ptr(), src.stride(), srcWBytes, srcH, cudaMemcpyHostToDevice, stream);

    conv3x3_1to8_cuda(in, tmp1, srcW, srcH, kernels, model::ARNet::kernelOffset[0], biases, model::ARNet::baisOffset[0], computeCapability, stream);
    conv3x3_8to8_cuda(tmp1, tmp2, srcW, srcH, kernels, model::ARNet::kernelOffset[1], biases, model::ARNet::baisOffset[1], computeCapability, stream);
    conv3x3_residual_8to8_cuda(tmp2, tmp1, srcW, srcH, kernels, model::ARNet::kernelOffset[2], biases, model::ARNet::baisOffset[2], computeCapability, stream);
    conv3x3_8to8_cuda(tmp1, tmp2, srcW, srcH, kernels, model::ARNet::kernelOffset[3], biases, model::ARNet::baisOffset[3], computeCapability, stream);
    conv3x3_residual_8to8_cuda(tmp2, tmp1, srcW, srcH, kernels, model::ARNet::kernelOffset[4], biases, model::ARNet::baisOffset[4], computeCapability, stream);
    conv3x3_8to8_cuda(tmp1, tmp2, srcW, srcH, kernels,+ model::ARNet::kernelOffset[5], biases, model::ARNet::baisOffset[5], computeCapability, stream);
    conv3x3_residual_8to8_cuda(tmp2, tmp1, srcW, srcH, kernels, model::ARNet::kernelOffset[6], biases, model::ARNet::baisOffset[6], computeCapability, stream);
    conv3x3_8to8_cuda(tmp1, tmp2, srcW, srcH, kernels, model::ARNet::kernelOffset[7], biases, model::ARNet::baisOffset[7], computeCapability, stream);
    conv3x3_residual_8to8_cuda(tmp2, tmp1, srcW, srcH, kernels, model::ARNet::kernelOffset[8], biases, model::ARNet::baisOffset[8], computeCapability, stream);
    deconv2x2_8to1_cuda(tmp1, out, dstW, dstH, kernels, model::ARNet::kernelOffset[9], dst.type(), computeCapability, stream);

    cudaMemcpy2DFromArrayAsync(dst.ptr(), dst.stride(), outArray, 0, 0, dstWBytes, dstH, cudaMemcpyDeviceToHost, stream);

    cudaStreamSynchronize(stream);

    cudaDestroyTextureObject(in);
    cudaDestroySurfaceObject(tmp1);
    cudaDestroySurfaceObject(tmp2);
    cudaDestroySurfaceObject(out);

    cudaFreeArray(inArray);
    cudaFreeArray(tmp1Array);
    cudaFreeArray(tmp2Array);
    cudaFreeArray(outArray);

    errors.local() = cudaGetLastError();
}

template<>
AC_EXPORT std::shared_ptr<ac::core::Processor> ac::core::Processor::create<ac::core::Processor::CUDA, ac::core::model::ARNet>(const int idx, const model::ARNet& model)
{
    return std::make_shared<cuda::CUDAProcessor<model::ARNet>>(idx, model);
}


template<>
AC_EXPORT const char* ac::core::Processor::info<ac::core::Processor::CUDA>()
{
    static auto infoBuffer = []() -> std::string {
        std::ostringstream buffer{ "CUDA:\n", std::ios_base::ate };
        for (int i = 0; i < ContextList.size(); i++)
            buffer << "  [" << i << "] " << ContextList[i].name << " (" << ContextList[i].vram << "MB, CC " << ContextList[i].computeCapability / 10.0 << ")" << '\n';
        return buffer.str();
    }();
    return infoBuffer.c_str();
}
